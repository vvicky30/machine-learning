{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "manual_neural_network_iris.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgtYD630hBK6uCEL1Dr1FV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvicky30/machine-learning/blob/master/manual_neural_network_iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4UYSQCrJMOq",
        "colab_type": "text"
      },
      "source": [
        "**Importing** **pakages** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGEepl6vqrl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu21S58YJlVL",
        "colab_type": "text"
      },
      "source": [
        "**data loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoYzUwWdFI0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('wine.csv')\n",
        "# print(df)\n",
        "a = pd.get_dummies(df['Wine'])\n",
        "df = pd.concat([df,a],axis=1)\n",
        "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
        "y = df[[1,2,3]].values\n",
        "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Y_test,test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKLT6K86Fjx1",
        "colab_type": "code",
        "outputId": "9ca9759f-3ec2-4643-ec13-3cd584a5d820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "df.head()#data exploration"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wine</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic.acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Acl</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid.phenols</th>\n",
              "      <th>Proanth</th>\n",
              "      <th>Color.int</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD</th>\n",
              "      <th>Proline</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
              "0     1    14.23        1.71  2.43  15.6  127  ...  1.04  3.92     1065  1  0  0\n",
              "1     1    13.20        1.78  2.14  11.2  100  ...  1.05  3.40     1050  1  0  0\n",
              "2     1    13.16        2.36  2.67  18.6  101  ...  1.03  3.17     1185  1  0  0\n",
              "3     1    14.37        1.95  2.50  16.8  113  ...  0.86  3.45     1480  1  0  0\n",
              "4     1    13.24        2.59  2.87  21.0  118  ...  1.04  2.93      735  1  0  0\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EnaaUA3FtaO",
        "colab_type": "code",
        "outputId": "057f3602-7b2b-47c0-f024-7a38d79dc0df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wine</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic.acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Acl</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid.phenols</th>\n",
              "      <th>Proanth</th>\n",
              "      <th>Color.int</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD</th>\n",
              "      <th>Proline</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>3</td>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>3</td>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.3</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>3</td>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.2</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>3</td>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.3</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>3</td>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.2</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
              "173     3    13.71        5.65  2.45  20.5   95  ...  0.64  1.74      740  0  0  1\n",
              "174     3    13.40        3.91  2.48  23.0  102  ...  0.70  1.56      750  0  0  1\n",
              "175     3    13.27        4.28  2.26  20.0  120  ...  0.59  1.56      835  0  0  1\n",
              "176     3    13.17        2.59  2.37  20.0  120  ...  0.60  1.62      840  0  0  1\n",
              "177     3    14.13        4.10  2.74  24.5   96  ...  0.61  1.60      560  0  0  1\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPu9ToBBFvq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(model,a0):\n",
        "    \n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
        "    # Do the first Linear step \n",
        "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
        "    z1 = a0.dot(W1) + b1\n",
        "    \n",
        "    # Put it through the first activation function\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # Second linear step\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    \n",
        "    # Second activation function\n",
        "    a2 = np.tanh(z2)\n",
        "    \n",
        "    #Third linear step\n",
        "    z3 = a2.dot(W3) + b3\n",
        "    \n",
        "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
        "    a3 = softmax(z3)\n",
        "    \n",
        "    #Store all results in these values\n",
        "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
        "    return cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaKZ0jM9JyJZ",
        "colab_type": "text"
      },
      "source": [
        "**softmax activation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lEASTAkGDrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#softmax activation function\n",
        "def softmax(z):\n",
        "    #Calculate exponent term first\n",
        "    exp_scores = np.exp(z)\n",
        "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Z02QMqKB3X",
        "colab_type": "text"
      },
      "source": [
        "**backprapogation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqe8qZ96Gkpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#backpropogation\n",
        "def backward_prop(model,cache,y):\n",
        "\n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
        "    \n",
        "    # Load forward propagation results\n",
        "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
        "    \n",
        "    # Get number of samples\n",
        "    m = y.shape[0]\n",
        "    \n",
        "    # Calculate loss derivative with respect to output\n",
        "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
        "\n",
        "    # Calculate loss derivative with respect to second layer weights\n",
        "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
        "    \n",
        "    # Calculate loss derivative with respect to second layer bias\n",
        "    db3 = 1/m*np.sum(dz3, axis=0)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer\n",
        "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer weights\n",
        "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer bias\n",
        "    db2 = 1/m*np.sum(dz2, axis=0)\n",
        "    \n",
        "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
        "    \n",
        "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
        "    \n",
        "    db1 = 1/m*np.sum(dz1,axis=0)\n",
        "    \n",
        "    # Store gradients\n",
        "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzPO-FS3KLyR",
        "colab_type": "text"
      },
      "source": [
        "**loss/objective/loss_function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtOcmfc5GsqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss/objective/loss_function\n",
        "def softmax_loss(y,y_hat):\n",
        "    # Clipping value\n",
        "    minval = 0.000000000001\n",
        "    # Number of samples\n",
        "    m = y.shape[0]\n",
        "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
        "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3S-rEsOKP3p",
        "colab_type": "text"
      },
      "source": [
        "**Loss and activation derivative for backpropagation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru6o504fG6Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loss and activation derivative for backpropagation\n",
        "def loss_derivative(y,y_hat):\n",
        "    return (y_hat-y)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return (1 - np.power(x, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBfK8urDKWQQ",
        "colab_type": "text"
      },
      "source": [
        "**Randomly initialize all Neural Network parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bODN9hvkHJK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Randomly initialize all Neural Network parameters\n",
        "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
        "    # First layer weights\n",
        "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
        "    \n",
        "    # First layer bias\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    \n",
        "    # Second layer weights\n",
        "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
        "    \n",
        "    # Second layer bias\n",
        "    b2 = np.zeros((1, nn_hdim))\n",
        "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
        "    b3 = np.zeros((1,nn_output_dim))\n",
        "    \n",
        "    \n",
        "    # Package and return model\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXDFJL5QKcTA",
        "colab_type": "text"
      },
      "source": [
        "**Update** **Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbMdrK-3HcaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Update Parameters\n",
        "\n",
        "def update_parameters(model,grads,learning_rate):\n",
        "    # Load parameters\n",
        "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
        "    \n",
        "    # Update parameters #decreasing by the some learning-rate*previous-grades\n",
        "    W1 -= learning_rate * grads['dW1']\n",
        "    b1 -= learning_rate * grads['db1']\n",
        "    W2 -= learning_rate * grads['dW2']\n",
        "    b2 -= learning_rate * grads['db2']\n",
        "    W3 -= learning_rate * grads['dW3']\n",
        "    b3 -= learning_rate * grads['db3']\n",
        "    \n",
        "    # Store and return parameters\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-tpZtDYK3Ao",
        "colab_type": "text"
      },
      "source": [
        "**Predict function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6vJSOXlHlvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict function\n",
        "\n",
        "def predict(model, x):\n",
        "    # Do forward pass\n",
        "    c = forward_prop(model,x)\n",
        "    #get y_hat\n",
        "    y_hat = np.argmax(c['a3'], axis=1)\n",
        "    return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVLNsoFSK-fh",
        "colab_type": "text"
      },
      "source": [
        "**train**-**function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8Aq1a4cHqLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train function\n",
        "losses=[]\n",
        "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
        "    # Gradient descent. Loop over epochs\n",
        "    for i in range(0, epochs):\n",
        "\n",
        "        # Forward propagation\n",
        "        cache = forward_prop(model,X_)\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = backward_prop(model,cache,y_)\n",
        "        \n",
        "        # Gradient descent parameter update\n",
        "        # Assign new parameters to the model\n",
        "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
        "    \n",
        "        # Pring loss & accuracy every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            a3 = cache['a3']\n",
        "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
        "            y_hat = predict(model,X_)\n",
        "            y_true = y_.argmax(axis=1)\n",
        "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
        "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZkoByAbLaRu",
        "colab_type": "text"
      },
      "source": [
        "**Initialize model parameters and train model on wine dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YgBwZqoHzjm",
        "colab_type": "code",
        "outputId": "9933b476-9832-46f7-c027-54f081793ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Initialize model parameters and train model on wine dataset\n",
        "\n",
        "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
        "model = train(model,X_train,Y_train,learning_rate=0.07,epochs=4500,print_loss=True)\n",
        "plt.plot(losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 : 2.3755326458076853\n",
            "Accuracy after iteration 0 : 16.19718309859155 %\n",
            "Loss after iteration 100 : 0.5556322523869474\n",
            "Accuracy after iteration 100 : 72.53521126760563 %\n",
            "Loss after iteration 200 : 0.44095679845373753\n",
            "Accuracy after iteration 200 : 80.28169014084507 %\n",
            "Loss after iteration 300 : 0.4014155624470504\n",
            "Accuracy after iteration 300 : 84.50704225352112 %\n",
            "Loss after iteration 400 : 0.3775290587136457\n",
            "Accuracy after iteration 400 : 89.43661971830986 %\n",
            "Loss after iteration 500 : 0.35454033007286856\n",
            "Accuracy after iteration 500 : 89.43661971830986 %\n",
            "Loss after iteration 600 : 0.3207389962770981\n",
            "Accuracy after iteration 600 : 89.43661971830986 %\n",
            "Loss after iteration 700 : 0.28761887527147306\n",
            "Accuracy after iteration 700 : 89.43661971830986 %\n",
            "Loss after iteration 800 : 0.2577735781573563\n",
            "Accuracy after iteration 800 : 90.14084507042254 %\n",
            "Loss after iteration 900 : 0.2371058183027732\n",
            "Accuracy after iteration 900 : 91.54929577464789 %\n",
            "Loss after iteration 1000 : 0.22190471816667454\n",
            "Accuracy after iteration 1000 : 91.54929577464789 %\n",
            "Loss after iteration 1100 : 0.21141983274618117\n",
            "Accuracy after iteration 1100 : 91.54929577464789 %\n",
            "Loss after iteration 1200 : 0.20294875914871544\n",
            "Accuracy after iteration 1200 : 92.25352112676056 %\n",
            "Loss after iteration 1300 : 0.19362345436149955\n",
            "Accuracy after iteration 1300 : 92.95774647887323 %\n",
            "Loss after iteration 1400 : 0.1757249685571216\n",
            "Accuracy after iteration 1400 : 93.66197183098592 %\n",
            "Loss after iteration 1500 : 0.16847563967459597\n",
            "Accuracy after iteration 1500 : 93.66197183098592 %\n",
            "Loss after iteration 1600 : 0.16482501029432825\n",
            "Accuracy after iteration 1600 : 93.66197183098592 %\n",
            "Loss after iteration 1700 : 0.16218253736713517\n",
            "Accuracy after iteration 1700 : 93.66197183098592 %\n",
            "Loss after iteration 1800 : 0.15993504410335663\n",
            "Accuracy after iteration 1800 : 93.66197183098592 %\n",
            "Loss after iteration 1900 : 0.1578399336065055\n",
            "Accuracy after iteration 1900 : 94.36619718309859 %\n",
            "Loss after iteration 2000 : 0.1558071704032692\n",
            "Accuracy after iteration 2000 : 94.36619718309859 %\n",
            "Loss after iteration 2100 : 0.15372721647778392\n",
            "Accuracy after iteration 2100 : 94.36619718309859 %\n",
            "Loss after iteration 2200 : 0.15150173861660787\n",
            "Accuracy after iteration 2200 : 94.36619718309859 %\n",
            "Loss after iteration 2300 : 0.14967987401582\n",
            "Accuracy after iteration 2300 : 94.36619718309859 %\n",
            "Loss after iteration 2400 : 0.14824774199766128\n",
            "Accuracy after iteration 2400 : 94.36619718309859 %\n",
            "Loss after iteration 2500 : 0.14707716480950409\n",
            "Accuracy after iteration 2500 : 94.36619718309859 %\n",
            "Loss after iteration 2600 : 0.1460647606291927\n",
            "Accuracy after iteration 2600 : 94.36619718309859 %\n",
            "Loss after iteration 2700 : 0.1451194283490791\n",
            "Accuracy after iteration 2700 : 94.36619718309859 %\n",
            "Loss after iteration 2800 : 0.14415405935996398\n",
            "Accuracy after iteration 2800 : 94.36619718309859 %\n",
            "Loss after iteration 2900 : 0.1430904449794138\n",
            "Accuracy after iteration 2900 : 94.36619718309859 %\n",
            "Loss after iteration 3000 : 0.14151126188889607\n",
            "Accuracy after iteration 3000 : 94.36619718309859 %\n",
            "Loss after iteration 3100 : 0.13748042482235173\n",
            "Accuracy after iteration 3100 : 95.07042253521126 %\n",
            "Loss after iteration 3200 : 0.13101734859440856\n",
            "Accuracy after iteration 3200 : 95.07042253521126 %\n",
            "Loss after iteration 3300 : 0.12206820696268986\n",
            "Accuracy after iteration 3300 : 95.77464788732394 %\n",
            "Loss after iteration 3400 : 0.11439382839228719\n",
            "Accuracy after iteration 3400 : 96.47887323943662 %\n",
            "Loss after iteration 3500 : 0.11204926748303284\n",
            "Accuracy after iteration 3500 : 96.47887323943662 %\n",
            "Loss after iteration 3600 : 0.11093714979328548\n",
            "Accuracy after iteration 3600 : 96.47887323943662 %\n",
            "Loss after iteration 3700 : 0.11010103162390376\n",
            "Accuracy after iteration 3700 : 96.47887323943662 %\n",
            "Loss after iteration 3800 : 0.10925837236904182\n",
            "Accuracy after iteration 3800 : 96.47887323943662 %\n",
            "Loss after iteration 3900 : 0.10795181722670885\n",
            "Accuracy after iteration 3900 : 96.47887323943662 %\n",
            "Loss after iteration 4000 : 0.10695722608650371\n",
            "Accuracy after iteration 4000 : 96.47887323943662 %\n",
            "Loss after iteration 4100 : 0.10649085936345445\n",
            "Accuracy after iteration 4100 : 96.47887323943662 %\n",
            "Loss after iteration 4200 : 0.10613441542030519\n",
            "Accuracy after iteration 4200 : 96.47887323943662 %\n",
            "Loss after iteration 4300 : 0.10578985663735355\n",
            "Accuracy after iteration 4300 : 96.47887323943662 %\n",
            "Loss after iteration 4400 : 0.10538811360881095\n",
            "Accuracy after iteration 4400 : 96.47887323943662 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8444c14f28>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoZbaFHfLmV0",
        "colab_type": "text"
      },
      "source": [
        "**calculating** **testiing** **accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0StoEYXIKse",
        "colab_type": "code",
        "outputId": "eae4e215-6ffa-443a-b0a6-60eb0796f93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Calculate testing accuracy\n",
        "test = predict(model,X_test)\n",
        "test = pd.get_dummies(test)\n",
        "Y_test = pd.DataFrame(Y_test)\n",
        "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy is:  97.22222222222221%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvBB4aRnoMNi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "af46a7f7-f61b-487a-ad8e-242314d7e102"
      },
      "source": [
        "#Reading data \n",
        "data=pd.read_csv(\"/content/iris.csv\")\n",
        "print(\"Describing the data: \",data.describe())\n",
        "print(\"Info of the data:\",data.info())"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Describing the data:                5.1         3.5         1.4         0.2\n",
            "count  149.000000  149.000000  149.000000  149.000000\n",
            "mean     5.848322    3.051007    3.774497    1.205369\n",
            "std      0.828594    0.433499    1.759651    0.761292\n",
            "min      4.300000    2.000000    1.000000    0.100000\n",
            "25%      5.100000    2.800000    1.600000    0.300000\n",
            "50%      5.800000    3.000000    4.400000    1.300000\n",
            "75%      6.400000    3.300000    5.100000    1.800000\n",
            "max      7.900000    4.400000    6.900000    2.500000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 149 entries, 0 to 148\n",
            "Data columns (total 5 columns):\n",
            "5.1            149 non-null float64\n",
            "3.5            149 non-null float64\n",
            "1.4            149 non-null float64\n",
            "0.2            149 non-null float64\n",
            "Iris-setosa    149 non-null object\n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 5.9+ KB\n",
            "Info of the data: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxuunXCqop75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns=[ 'SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm','class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KPpM2OVo3Hu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "3addb38b-06fa-4e1d-fdd0-3283af4e0ebf"
      },
      "source": [
        "data"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>149 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm           class\n",
              "0              4.9           3.0            1.4           0.2     Iris-setosa\n",
              "1              4.7           3.2            1.3           0.2     Iris-setosa\n",
              "2              4.6           3.1            1.5           0.2     Iris-setosa\n",
              "3              5.0           3.6            1.4           0.2     Iris-setosa\n",
              "4              5.4           3.9            1.7           0.4     Iris-setosa\n",
              "..             ...           ...            ...           ...             ...\n",
              "144            6.7           3.0            5.2           2.3  Iris-virginica\n",
              "145            6.3           2.5            5.0           1.9  Iris-virginica\n",
              "146            6.5           3.0            5.2           2.0  Iris-virginica\n",
              "147            6.2           3.4            5.4           2.3  Iris-virginica\n",
              "148            5.9           3.0            5.1           1.8  Iris-virginica\n",
              "\n",
              "[149 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-1ios09sX_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a= pd.get_dummies(data['class'])\n",
        "data = pd.concat([data,a],axis=1,sort=False)\n",
        "#df = pd.read_csv('wine.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIlBJ3gAvGTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "1db27f45-8d66-4a7b-b78a-1fec285624b0"
      },
      "source": [
        "data"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>class</th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>149 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     SepalLengthCm  SepalWidthCm  ...  Iris-versicolor  Iris-virginica\n",
              "0              4.9           3.0  ...                0               0\n",
              "1              4.7           3.2  ...                0               0\n",
              "2              4.6           3.1  ...                0               0\n",
              "3              5.0           3.6  ...                0               0\n",
              "4              5.4           3.9  ...                0               0\n",
              "..             ...           ...  ...              ...             ...\n",
              "144            6.7           3.0  ...                0               1\n",
              "145            6.3           2.5  ...                0               1\n",
              "146            6.5           3.0  ...                0               1\n",
              "147            6.2           3.4  ...                0               1\n",
              "148            5.9           3.0  ...                0               1\n",
              "\n",
              "[149 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R90nmsP5ypYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#division of data into features and labels\n",
        "x = data.drop([\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\",'class'],axis=1)\n",
        "y = data[[\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"]].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZm5qs8CyzbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dividing into features and labels\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x,y,train_size=0.8)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Newrd5x7zO3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e8040eb-cf87-41ec-ddd0-1c101d04ff1e"
      },
      "source": [
        "model = initialize_parameters(nn_input_dim=4, nn_hdim= 5, nn_output_dim= 3)\n",
        "model = train(model,X_train,Y_train,learning_rate=0.09,epochs=4500,print_loss=True)\n",
        "plt.plot(losses)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 : 1.5335504584825586\n",
            "Accuracy after iteration 0 : 21.008403361344538 %\n",
            "Loss after iteration 100 : 0.46853067578560936\n",
            "Accuracy after iteration 100 : 83.19327731092437 %\n",
            "Loss after iteration 200 : 0.3597823919514872\n",
            "Accuracy after iteration 200 : 88.23529411764706 %\n",
            "Loss after iteration 300 : 0.2869922700476085\n",
            "Accuracy after iteration 300 : 90.75630252100841 %\n",
            "Loss after iteration 400 : 0.21492083598164005\n",
            "Accuracy after iteration 400 : 93.27731092436974 %\n",
            "Loss after iteration 500 : 0.1638702185959495\n",
            "Accuracy after iteration 500 : 93.27731092436974 %\n",
            "Loss after iteration 600 : 0.13141180624399929\n",
            "Accuracy after iteration 600 : 94.9579831932773 %\n",
            "Loss after iteration 700 : 0.1094088840784753\n",
            "Accuracy after iteration 700 : 96.63865546218487 %\n",
            "Loss after iteration 800 : 0.0926918452440252\n",
            "Accuracy after iteration 800 : 97.47899159663865 %\n",
            "Loss after iteration 900 : 0.07962642298706671\n",
            "Accuracy after iteration 900 : 98.31932773109243 %\n",
            "Loss after iteration 1000 : 0.0698082550630545\n",
            "Accuracy after iteration 1000 : 98.31932773109243 %\n",
            "Loss after iteration 1100 : 0.06265134613433222\n",
            "Accuracy after iteration 1100 : 98.31932773109243 %\n",
            "Loss after iteration 1200 : 0.05739416439395458\n",
            "Accuracy after iteration 1200 : 98.31932773109243 %\n",
            "Loss after iteration 1300 : 0.053366121603399416\n",
            "Accuracy after iteration 1300 : 98.31932773109243 %\n",
            "Loss after iteration 1400 : 0.05011489935839757\n",
            "Accuracy after iteration 1400 : 98.31932773109243 %\n",
            "Loss after iteration 1500 : 0.04733426585853482\n",
            "Accuracy after iteration 1500 : 98.31932773109243 %\n",
            "Loss after iteration 1600 : 0.04452847328112226\n",
            "Accuracy after iteration 1600 : 98.31932773109243 %\n",
            "Loss after iteration 1700 : 0.038001221794629836\n",
            "Accuracy after iteration 1700 : 99.15966386554622 %\n",
            "Loss after iteration 1800 : 0.03365447870082182\n",
            "Accuracy after iteration 1800 : 99.15966386554622 %\n",
            "Loss after iteration 1900 : 0.02876644973766425\n",
            "Accuracy after iteration 1900 : 99.15966386554622 %\n",
            "Loss after iteration 2000 : 0.02341606969843689\n",
            "Accuracy after iteration 2000 : 99.15966386554622 %\n",
            "Loss after iteration 2100 : 0.019668346673394097\n",
            "Accuracy after iteration 2100 : 100.0 %\n",
            "Loss after iteration 2200 : 0.01696617101023032\n",
            "Accuracy after iteration 2200 : 100.0 %\n",
            "Loss after iteration 2300 : 0.014833995551745806\n",
            "Accuracy after iteration 2300 : 100.0 %\n",
            "Loss after iteration 2400 : 0.013121599933207833\n",
            "Accuracy after iteration 2400 : 100.0 %\n",
            "Loss after iteration 2500 : 0.01180129029915518\n",
            "Accuracy after iteration 2500 : 100.0 %\n",
            "Loss after iteration 2600 : 0.010790144904509743\n",
            "Accuracy after iteration 2600 : 100.0 %\n",
            "Loss after iteration 2700 : 0.009979217815142133\n",
            "Accuracy after iteration 2700 : 100.0 %\n",
            "Loss after iteration 2800 : 0.009293469512276076\n",
            "Accuracy after iteration 2800 : 100.0 %\n",
            "Loss after iteration 2900 : 0.008692574013260625\n",
            "Accuracy after iteration 2900 : 100.0 %\n",
            "Loss after iteration 3000 : 0.008155939973750677\n",
            "Accuracy after iteration 3000 : 100.0 %\n",
            "Loss after iteration 3100 : 0.0076724501836938205\n",
            "Accuracy after iteration 3100 : 100.0 %\n",
            "Loss after iteration 3200 : 0.007235211373992505\n",
            "Accuracy after iteration 3200 : 100.0 %\n",
            "Loss after iteration 3300 : 0.006839096236339871\n",
            "Accuracy after iteration 3300 : 100.0 %\n",
            "Loss after iteration 3400 : 0.006479707692329664\n",
            "Accuracy after iteration 3400 : 100.0 %\n",
            "Loss after iteration 3500 : 0.006153033415772369\n",
            "Accuracy after iteration 3500 : 100.0 %\n",
            "Loss after iteration 3600 : 0.0058553881770499975\n",
            "Accuracy after iteration 3600 : 100.0 %\n",
            "Loss after iteration 3700 : 0.005583437439471247\n",
            "Accuracy after iteration 3700 : 100.0 %\n",
            "Loss after iteration 3800 : 0.005334216984618184\n",
            "Accuracy after iteration 3800 : 100.0 %\n",
            "Loss after iteration 3900 : 0.005105127523682927\n",
            "Accuracy after iteration 3900 : 100.0 %\n",
            "Loss after iteration 4000 : 0.00489390871708366\n",
            "Accuracy after iteration 4000 : 100.0 %\n",
            "Loss after iteration 4100 : 0.004698602482353458\n",
            "Accuracy after iteration 4100 : 100.0 %\n",
            "Loss after iteration 4200 : 0.004517513530234031\n",
            "Accuracy after iteration 4200 : 100.0 %\n",
            "Loss after iteration 4300 : 0.004349171805534118\n",
            "Accuracy after iteration 4300 : 100.0 %\n",
            "Loss after iteration 4400 : 0.004192298969039885\n",
            "Accuracy after iteration 4400 : 100.0 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8406374748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQUruIiIZ9P8BAXUo1lqz55oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfFajWFgzm67",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2a82e3e-a30a-4109-8f7c-bfbb4b6c411f"
      },
      "source": [
        "test = predict(model,X_test)\n",
        "test = pd.get_dummies(test)\n",
        "Y_test = pd.DataFrame(Y_test)\n",
        "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy is:  90.0%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}