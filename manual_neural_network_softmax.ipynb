{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "manual_neural_network_softmax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9iwyfAHSwMoJ4Ky5lpcF0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvicky30/machine-learning/blob/master/manual_neural_network_softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4UYSQCrJMOq",
        "colab_type": "text"
      },
      "source": [
        "**Importing** **pakages** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGEepl6vqrl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu21S58YJlVL",
        "colab_type": "text"
      },
      "source": [
        "**data loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoYzUwWdFI0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('wine.csv')\n",
        "# print(df)\n",
        "a = pd.get_dummies(df['Wine'])\n",
        "df = pd.concat([df,a],axis=1)\n",
        "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
        "y = df[[1,2,3]].values\n",
        "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Y_test,test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKLT6K86Fjx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "9ca9759f-3ec2-4643-ec13-3cd584a5d820"
      },
      "source": [
        "df.head()#data exploration"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wine</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic.acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Acl</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid.phenols</th>\n",
              "      <th>Proanth</th>\n",
              "      <th>Color.int</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD</th>\n",
              "      <th>Proline</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
              "0     1    14.23        1.71  2.43  15.6  127  ...  1.04  3.92     1065  1  0  0\n",
              "1     1    13.20        1.78  2.14  11.2  100  ...  1.05  3.40     1050  1  0  0\n",
              "2     1    13.16        2.36  2.67  18.6  101  ...  1.03  3.17     1185  1  0  0\n",
              "3     1    14.37        1.95  2.50  16.8  113  ...  0.86  3.45     1480  1  0  0\n",
              "4     1    13.24        2.59  2.87  21.0  118  ...  1.04  2.93      735  1  0  0\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EnaaUA3FtaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "057f3602-7b2b-47c0-f024-7a38d79dc0df"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wine</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic.acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Acl</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid.phenols</th>\n",
              "      <th>Proanth</th>\n",
              "      <th>Color.int</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD</th>\n",
              "      <th>Proline</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>3</td>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>3</td>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.3</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>3</td>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.2</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>3</td>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.3</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>3</td>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.2</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
              "173     3    13.71        5.65  2.45  20.5   95  ...  0.64  1.74      740  0  0  1\n",
              "174     3    13.40        3.91  2.48  23.0  102  ...  0.70  1.56      750  0  0  1\n",
              "175     3    13.27        4.28  2.26  20.0  120  ...  0.59  1.56      835  0  0  1\n",
              "176     3    13.17        2.59  2.37  20.0  120  ...  0.60  1.62      840  0  0  1\n",
              "177     3    14.13        4.10  2.74  24.5   96  ...  0.61  1.60      560  0  0  1\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPu9ToBBFvq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(model,a0):\n",
        "    \n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
        "    # Do the first Linear step \n",
        "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
        "    z1 = a0.dot(W1) + b1\n",
        "    \n",
        "    # Put it through the first activation function\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # Second linear step\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    \n",
        "    # Second activation function\n",
        "    a2 = np.tanh(z2)\n",
        "    \n",
        "    #Third linear step\n",
        "    z3 = a2.dot(W3) + b3\n",
        "    \n",
        "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
        "    a3 = softmax(z3)\n",
        "    \n",
        "    #Store all results in these values\n",
        "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
        "    return cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaKZ0jM9JyJZ",
        "colab_type": "text"
      },
      "source": [
        "**softmax activation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lEASTAkGDrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#softmax activation function\n",
        "def softmax(z):\n",
        "    #Calculate exponent term first\n",
        "    exp_scores = np.exp(z)\n",
        "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Z02QMqKB3X",
        "colab_type": "text"
      },
      "source": [
        "**backprapogation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqe8qZ96Gkpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#backpropogation\n",
        "def backward_prop(model,cache,y):\n",
        "\n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
        "    \n",
        "    # Load forward propagation results\n",
        "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
        "    \n",
        "    # Get number of samples\n",
        "    m = y.shape[0]\n",
        "    \n",
        "    # Calculate loss derivative with respect to output\n",
        "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
        "\n",
        "    # Calculate loss derivative with respect to second layer weights\n",
        "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
        "    \n",
        "    # Calculate loss derivative with respect to second layer bias\n",
        "    db3 = 1/m*np.sum(dz3, axis=0)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer\n",
        "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer weights\n",
        "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer bias\n",
        "    db2 = 1/m*np.sum(dz2, axis=0)\n",
        "    \n",
        "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
        "    \n",
        "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
        "    \n",
        "    db1 = 1/m*np.sum(dz1,axis=0)\n",
        "    \n",
        "    # Store gradients\n",
        "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzPO-FS3KLyR",
        "colab_type": "text"
      },
      "source": [
        "**loss/objective/loss_function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtOcmfc5GsqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss/objective/loss_function\n",
        "def softmax_loss(y,y_hat):\n",
        "    # Clipping value\n",
        "    minval = 0.000000000001\n",
        "    # Number of samples\n",
        "    m = y.shape[0]\n",
        "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
        "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3S-rEsOKP3p",
        "colab_type": "text"
      },
      "source": [
        "**Loss and activation derivative for backpropagation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru6o504fG6Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loss and activation derivative for backpropagation\n",
        "def loss_derivative(y,y_hat):\n",
        "    return (y_hat-y)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return (1 - np.power(x, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBfK8urDKWQQ",
        "colab_type": "text"
      },
      "source": [
        "**Randomly initialize all Neural Network parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bODN9hvkHJK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Randomly initialize all Neural Network parameters\n",
        "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
        "    # First layer weights\n",
        "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
        "    \n",
        "    # First layer bias\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    \n",
        "    # Second layer weights\n",
        "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
        "    \n",
        "    # Second layer bias\n",
        "    b2 = np.zeros((1, nn_hdim))\n",
        "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
        "    b3 = np.zeros((1,nn_output_dim))\n",
        "    \n",
        "    \n",
        "    # Package and return model\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXDFJL5QKcTA",
        "colab_type": "text"
      },
      "source": [
        "**Update** **Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbMdrK-3HcaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Update Parameters\n",
        "\n",
        "def update_parameters(model,grads,learning_rate):\n",
        "    # Load parameters\n",
        "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
        "    \n",
        "    # Update parameters #decreasing by the some learning-rate*previous-grades\n",
        "    W1 -= learning_rate * grads['dW1']\n",
        "    b1 -= learning_rate * grads['db1']\n",
        "    W2 -= learning_rate * grads['dW2']\n",
        "    b2 -= learning_rate * grads['db2']\n",
        "    W3 -= learning_rate * grads['dW3']\n",
        "    b3 -= learning_rate * grads['db3']\n",
        "    \n",
        "    # Store and return parameters\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-tpZtDYK3Ao",
        "colab_type": "text"
      },
      "source": [
        "**Predict function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6vJSOXlHlvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict function\n",
        "\n",
        "def predict(model, x):\n",
        "    # Do forward pass\n",
        "    c = forward_prop(model,x)\n",
        "    #get y_hat\n",
        "    y_hat = np.argmax(c['a3'], axis=1)\n",
        "    return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVLNsoFSK-fh",
        "colab_type": "text"
      },
      "source": [
        "**train**-**function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8Aq1a4cHqLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train function\n",
        "losses=[]\n",
        "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
        "    # Gradient descent. Loop over epochs\n",
        "    for i in range(0, epochs):\n",
        "\n",
        "        # Forward propagation\n",
        "        cache = forward_prop(model,X_)\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = backward_prop(model,cache,y_)\n",
        "        \n",
        "        # Gradient descent parameter update\n",
        "        # Assign new parameters to the model\n",
        "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
        "    \n",
        "        # Pring loss & accuracy every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            a3 = cache['a3']\n",
        "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
        "            y_hat = predict(model,X_)\n",
        "            y_true = y_.argmax(axis=1)\n",
        "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
        "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZkoByAbLaRu",
        "colab_type": "text"
      },
      "source": [
        "**Initialize model parameters and train model on wine dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YgBwZqoHzjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9933b476-9832-46f7-c027-54f081793ad5"
      },
      "source": [
        "#Initialize model parameters and train model on wine dataset\n",
        "\n",
        "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
        "model = train(model,X_train,Y_train,learning_rate=0.07,epochs=4500,print_loss=True)\n",
        "plt.plot(losses)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 : 2.3755326458076853\n",
            "Accuracy after iteration 0 : 16.19718309859155 %\n",
            "Loss after iteration 100 : 0.5556322523869474\n",
            "Accuracy after iteration 100 : 72.53521126760563 %\n",
            "Loss after iteration 200 : 0.44095679845373753\n",
            "Accuracy after iteration 200 : 80.28169014084507 %\n",
            "Loss after iteration 300 : 0.4014155624470504\n",
            "Accuracy after iteration 300 : 84.50704225352112 %\n",
            "Loss after iteration 400 : 0.3775290587136457\n",
            "Accuracy after iteration 400 : 89.43661971830986 %\n",
            "Loss after iteration 500 : 0.35454033007286856\n",
            "Accuracy after iteration 500 : 89.43661971830986 %\n",
            "Loss after iteration 600 : 0.3207389962770981\n",
            "Accuracy after iteration 600 : 89.43661971830986 %\n",
            "Loss after iteration 700 : 0.28761887527147306\n",
            "Accuracy after iteration 700 : 89.43661971830986 %\n",
            "Loss after iteration 800 : 0.2577735781573563\n",
            "Accuracy after iteration 800 : 90.14084507042254 %\n",
            "Loss after iteration 900 : 0.2371058183027732\n",
            "Accuracy after iteration 900 : 91.54929577464789 %\n",
            "Loss after iteration 1000 : 0.22190471816667454\n",
            "Accuracy after iteration 1000 : 91.54929577464789 %\n",
            "Loss after iteration 1100 : 0.21141983274618117\n",
            "Accuracy after iteration 1100 : 91.54929577464789 %\n",
            "Loss after iteration 1200 : 0.20294875914871544\n",
            "Accuracy after iteration 1200 : 92.25352112676056 %\n",
            "Loss after iteration 1300 : 0.19362345436149955\n",
            "Accuracy after iteration 1300 : 92.95774647887323 %\n",
            "Loss after iteration 1400 : 0.1757249685571216\n",
            "Accuracy after iteration 1400 : 93.66197183098592 %\n",
            "Loss after iteration 1500 : 0.16847563967459597\n",
            "Accuracy after iteration 1500 : 93.66197183098592 %\n",
            "Loss after iteration 1600 : 0.16482501029432825\n",
            "Accuracy after iteration 1600 : 93.66197183098592 %\n",
            "Loss after iteration 1700 : 0.16218253736713517\n",
            "Accuracy after iteration 1700 : 93.66197183098592 %\n",
            "Loss after iteration 1800 : 0.15993504410335663\n",
            "Accuracy after iteration 1800 : 93.66197183098592 %\n",
            "Loss after iteration 1900 : 0.1578399336065055\n",
            "Accuracy after iteration 1900 : 94.36619718309859 %\n",
            "Loss after iteration 2000 : 0.1558071704032692\n",
            "Accuracy after iteration 2000 : 94.36619718309859 %\n",
            "Loss after iteration 2100 : 0.15372721647778392\n",
            "Accuracy after iteration 2100 : 94.36619718309859 %\n",
            "Loss after iteration 2200 : 0.15150173861660787\n",
            "Accuracy after iteration 2200 : 94.36619718309859 %\n",
            "Loss after iteration 2300 : 0.14967987401582\n",
            "Accuracy after iteration 2300 : 94.36619718309859 %\n",
            "Loss after iteration 2400 : 0.14824774199766128\n",
            "Accuracy after iteration 2400 : 94.36619718309859 %\n",
            "Loss after iteration 2500 : 0.14707716480950409\n",
            "Accuracy after iteration 2500 : 94.36619718309859 %\n",
            "Loss after iteration 2600 : 0.1460647606291927\n",
            "Accuracy after iteration 2600 : 94.36619718309859 %\n",
            "Loss after iteration 2700 : 0.1451194283490791\n",
            "Accuracy after iteration 2700 : 94.36619718309859 %\n",
            "Loss after iteration 2800 : 0.14415405935996398\n",
            "Accuracy after iteration 2800 : 94.36619718309859 %\n",
            "Loss after iteration 2900 : 0.1430904449794138\n",
            "Accuracy after iteration 2900 : 94.36619718309859 %\n",
            "Loss after iteration 3000 : 0.14151126188889607\n",
            "Accuracy after iteration 3000 : 94.36619718309859 %\n",
            "Loss after iteration 3100 : 0.13748042482235173\n",
            "Accuracy after iteration 3100 : 95.07042253521126 %\n",
            "Loss after iteration 3200 : 0.13101734859440856\n",
            "Accuracy after iteration 3200 : 95.07042253521126 %\n",
            "Loss after iteration 3300 : 0.12206820696268986\n",
            "Accuracy after iteration 3300 : 95.77464788732394 %\n",
            "Loss after iteration 3400 : 0.11439382839228719\n",
            "Accuracy after iteration 3400 : 96.47887323943662 %\n",
            "Loss after iteration 3500 : 0.11204926748303284\n",
            "Accuracy after iteration 3500 : 96.47887323943662 %\n",
            "Loss after iteration 3600 : 0.11093714979328548\n",
            "Accuracy after iteration 3600 : 96.47887323943662 %\n",
            "Loss after iteration 3700 : 0.11010103162390376\n",
            "Accuracy after iteration 3700 : 96.47887323943662 %\n",
            "Loss after iteration 3800 : 0.10925837236904182\n",
            "Accuracy after iteration 3800 : 96.47887323943662 %\n",
            "Loss after iteration 3900 : 0.10795181722670885\n",
            "Accuracy after iteration 3900 : 96.47887323943662 %\n",
            "Loss after iteration 4000 : 0.10695722608650371\n",
            "Accuracy after iteration 4000 : 96.47887323943662 %\n",
            "Loss after iteration 4100 : 0.10649085936345445\n",
            "Accuracy after iteration 4100 : 96.47887323943662 %\n",
            "Loss after iteration 4200 : 0.10613441542030519\n",
            "Accuracy after iteration 4200 : 96.47887323943662 %\n",
            "Loss after iteration 4300 : 0.10578985663735355\n",
            "Accuracy after iteration 4300 : 96.47887323943662 %\n",
            "Loss after iteration 4400 : 0.10538811360881095\n",
            "Accuracy after iteration 4400 : 96.47887323943662 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8444c14f28>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoZbaFHfLmV0",
        "colab_type": "text"
      },
      "source": [
        "**calculating** **testiing** **accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0StoEYXIKse",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eae4e215-6ffa-443a-b0a6-60eb0796f93e"
      },
      "source": [
        "#Calculate testing accuracy\n",
        "test = predict(model,X_test)\n",
        "test = pd.get_dummies(test)\n",
        "Y_test = pd.DataFrame(Y_test)\n",
        "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy is:  97.22222222222221%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}